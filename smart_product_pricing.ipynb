{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oUNbo--9F8XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG5Yw9O7GHg3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o08AVLcdGR_h"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/aiml_challenge/dataset/train.csv'\n",
        "df = pd.read_csv(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PixX874AGSby"
      },
      "outputs": [],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBN_sXepGSfE"
      },
      "outputs": [],
      "source": [
        "df['value'] = df['catalog_content'].str.extract(r'Value\\s*:\\s*([0-9.]+)')\n",
        "df['unit'] = df['catalog_content'].str.extract(r'Unit\\s*:\\s*([A-Za-z]+)')\n",
        "\n",
        "# Remove value and unit parts from the text to create 'text' column\n",
        "df['text'] = df['catalog_content'].str.replace(r'Value\\s*:\\s*[0-9.]+\\s*,?\\s*', '', regex=True)\n",
        "df['text'] = df['text'].str.replace(r'Unit\\s*:\\s*[A-Za-z]+\\s*,?\\s*', '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGmBRefHU1xC"
      },
      "outputs": [],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l_6zcpkU2S1"
      },
      "outputs": [],
      "source": [
        "# df['text'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlO-5BUCVU0o"
      },
      "outputs": [],
      "source": [
        "df['unit'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd65f614"
      },
      "outputs": [],
      "source": [
        "# x.to_csv('df_with_units.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nshtOjllYT6W"
      },
      "outputs": [],
      "source": [
        "# df[df['unit'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwEYHr6lY4to"
      },
      "outputs": [],
      "source": [
        "df['unit'] = df['unit'].str.strip().str.lower()\n",
        "unit_map = {\n",
        "    # weight\n",
        "    'gram': 'g', 'grams': 'g', 'gramm': 'g', 'gr': 'g', 'gram': 'g', 'grams': 'g',\n",
        "    'kg': 'kg', 'pound': 'lb', 'pounds': 'lb', 'lb': 'lb', 'lbs': 'lb',\n",
        "\n",
        "    # volume\n",
        "    'liter': 'l', 'liters': 'l', 'ltr': 'l',\n",
        "    'milliliter': 'ml', 'millilitre': 'ml', 'mililitro': 'ml',\n",
        "    'fl': 'fl_oz', 'fluid': 'fl_oz', 'fluid ounce': 'fl_oz', 'ounce': 'oz', 'ounces': 'oz', 'oz': 'oz', 'ounc': 'oz',\n",
        "\n",
        "    # countable units\n",
        "    'count': 'count', 'each': 'count', 'ct': 'count', 'piece': 'count', 'pack': 'pack', 'packs': 'pack',\n",
        "    'unit': 'count', 'units': 'count', 'product': 'count', 'bottle': 'count', 'bottles': 'count',\n",
        "    'box': 'count', 'boxes': 'count', 'bag': 'count', 'bags': 'count', 'jar': 'count', 'can': 'count', 'carton': 'count',\n",
        "    'case': 'count', 'bucket': 'count', 'pouch': 'count', 'ziplock': 'count', 'paper': 'count',\n",
        "\n",
        "    # area/length\n",
        "    'sq': 'sq_ft', 'foot': 'sq_ft', 'cm': 'cm', 'in': 'inch', 'k': 'count', 'tea': 'count', 'comes': 'count', 'per': 'count'\n",
        "}\n",
        "\n",
        "df['unit_normalized'] = df['unit'].map(unit_map).fillna('unknown')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
        "\n",
        "# Calculate the median for 'value' and unit (mode for unit) columns\n",
        "value_median = df['value'].median()\n",
        "unit_median = df['unit'].mode()[0] # Using mode for the most frequent unit\n",
        "\n",
        "# Fill null values with the calculated medians\n",
        "df['value'].fillna(value_median, inplace=True)\n",
        "df['unit'].fillna(unit_median, inplace=True)"
      ],
      "metadata": {
        "id": "T6zlfIkxTyRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anAIqqokfRmg"
      },
      "outputs": [],
      "source": [
        "def convert_to_base(quantity, unit):\n",
        "    if unit == 'kg':\n",
        "        return quantity * 1000, 'g'\n",
        "    elif unit == 'lb':\n",
        "        return quantity * 453.592, 'g'\n",
        "    elif unit == 'oz':\n",
        "        return quantity * 28.3495, 'g'\n",
        "    elif unit == 'fl_oz':\n",
        "        return quantity * 29.5735, 'ml'\n",
        "    elif unit == 'l':\n",
        "        return quantity * 1000, 'ml'\n",
        "    else:\n",
        "        return quantity, unit\n",
        "\n",
        "# Convert the 'value' column to numeric, coercing errors to NaN\n",
        "df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
        "\n",
        "\n",
        "df[['quantity_converted', 'unit_base']] = df.apply(\n",
        "    lambda row: pd.Series(convert_to_base(row['value'], row['unit_normalized'])), axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4VIoCvmf2T2"
      },
      "outputs": [],
      "source": [
        "df['unit_base'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQFURdWq6mQo"
      },
      "outputs": [],
      "source": [
        "Df = df[['text', 'quantity_converted', 'unit_base','price',]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4N8YQ8y7MuF"
      },
      "outputs": [],
      "source": [
        "Df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dvcvt207S0q"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "Df['unit_base_encoded'] = label_encoder.fit_transform(Df['unit_base'])\n",
        "Df = Df.drop(columns = ['unit_base'],axis =1)\n",
        "display(Df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "_FZLZEvAUL2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(x):\n",
        "    x = re.sub(r'[\\n\\r\\t]+', ' ', x)          # remove newlines/tabs\n",
        "    x = x.translate(str.maketrans('', '', string.punctuation))  # remove punctuations\n",
        "    x = x.lower()                             # lowercase\n",
        "    x = ' '.join([lem.lemmatize(w) for w in x.split() if w not in stop])  # lemmatize + remove stopwords\n",
        "    return x\n",
        "\n",
        "Df['text'] = Df['text'].astype(str).apply(clean_text)\n"
      ],
      "metadata": {
        "id": "WtkK8ReFPLdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a444c9bb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T0iNb6rDW4d"
      },
      "outputs": [],
      "source": [
        "# print(df['text'][3])\n",
        "# print(Df['text'][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch7y74DCExDO"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(Df['price'], bins=50, kde=True, edgecolor='black')\n",
        "# plt.xlabel('Price')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.title('Distribution of Price with KDE')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.head()"
      ],
      "metadata": {
        "id": "Xd5i_XPcIAxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Select features\n",
        "X = Df[['text', 'quantity_converted', 'unit_base_encoded']]\n",
        "y = Df['price'] # Use the log of the price for y (standard approach)\n",
        "\n",
        "Df['log_price'] = np.log1p(y)\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
        "\n",
        "# Create a ColumnTransformer to apply different transformations to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', TfidfVectorizer(max_features=500000, ngram_range=(1,2),stop_words='english'), 'text'),\n",
        "        ('numerical', StandardScaler(), ['quantity_converted'])\n",
        "    ],\n",
        "    remainder='passthrough' # Keep other columns (if any)\n",
        ")\n",
        "\n",
        "# Create the model pipeline\n",
        "model = make_pipeline(\n",
        "    preprocessor,\n",
        "    Ridge(alpha=1.0)\n",
        ")\n",
        "\n",
        "# Train the model using the log price\n",
        "model.fit(X_train, Df.loc[X_train.index, 'log_price'])\n",
        "\n"
      ],
      "metadata": {
        "id": "hydKs-h5PoqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DY9r7jDnPosR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyutPCYuBveQ"
      },
      "outputs": [],
      "source": [
        "# Make predictions (on log_price scale)\n",
        "preds_log = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions back to the original price scale\n",
        "preds = np.expm1(preds_log)\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Evaluate the model using original y_test and inverse-transformed predictions\n",
        "mae = mean_absolute_error(y_test, preds)\n",
        "mse = mean_squared_error(y_test, preds)\n",
        "rmse = np.sqrt(mse) # Calculate RMSE by taking the square root of MSE\n",
        "r2 = r2_score(y_test, preds)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "print(f'R-squared (R2): {r2}')"
      ],
      "metadata": {
        "id": "89gIFQjhUnKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk_fW1AvCLDP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate the model using original y_test and inverse-transformed predictions\n",
        "mae = mean_absolute_error(y_test, preds)\n",
        "mse = mean_squared_error(y_test, preds)\n",
        "rmse = np.sqrt(mse) # Calculate RMSE by taking the square root of MSE\n",
        "r2 = r2_score(y_test, preds)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "print(f'R-squared (R2): {r2}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def smape(y_true, y_pred):\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "\n",
        "    return np.mean(numerator / denominator) * 100\n",
        "\n",
        "# Calculate SMAPE\n",
        "smape_score = smape(y_test, preds)\n",
        "print(f'SMAPE: {smape_score}')"
      ],
      "metadata": {
        "id": "2gc0yzZxPouw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRqZQcQCZwj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('/content/drive/MyDrive/aiml_challenge/dataset/test.csv')"
      ],
      "metadata": {
        "id": "tvNXVD3iPoxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the same preprocessing steps to the test dataframe\n",
        "\n",
        "# Extract value and unit\n",
        "test_df['value'] = test_df['catalog_content'].str.extract(r'Value\\s*:\\s*([0-9.]+)')\n",
        "test_df['unit'] = test_df['catalog_content'].str.extract(r'Unit\\s*:\\s*([A-Za-z]+)')\n",
        "\n",
        "# Remove value and unit parts from the text to create 'text' column\n",
        "test_df['text'] = test_df['catalog_content'].str.replace(r'Value\\s*:\\s*[0-9.]+\\s*,?\\s*', '', regex=True)\n",
        "test_df['text'] = test_df['text'].str.replace(r'Unit\\s*:\\s*[A-Za-z]+\\s*,?\\s*', '', regex=True)\n",
        "\n",
        "# Standardize unit column\n",
        "test_df['unit'] = test_df['unit'].str.strip().str.lower()\n",
        "# Use the same unit_map from the training data\n",
        "test_df['unit_normalized'] = test_df['unit'].map(unit_map).fillna('unknown')\n",
        "\n",
        "# Convert to base units\n",
        "# Convert the 'value' column to numeric, coercing errors to NaN\n",
        "test_df['value'] = pd.to_numeric(test_df['value'], errors='coerce')\n",
        "\n",
        "# Impute null values in 'value' and 'unit' using medians/modes from training data\n",
        "# Ensure value_median and unit_median are calculated from the training data (df) before this cell is run\n",
        "test_df['value'].fillna(value_median, inplace=True)\n",
        "test_df['unit'].fillna(unit_median, inplace=True)\n",
        "\n",
        "\n",
        "test_df[['quantity_converted', 'unit_base']] = test_df.apply(\n",
        "    lambda row: pd.Series(convert_to_base(row['value'], row['unit_normalized'])), axis=1\n",
        ")\n",
        "\n",
        "# Encode unit_base using the same LabelEncoder fitted on the training data\n",
        "# Need to recreate and fit the label encoder on the combined data or save and load the fitted encoder\n",
        "# For simplicity here, refit on training data and transform test data (assuming same categories)\n",
        "# A robust approach would save/load the fitted encoder or fit on combined unique values\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(df['unit_base']) # Fit on the unit_base from the training df\n",
        "test_df['unit_base_encoded'] = label_encoder.transform(test_df['unit_base'])\n",
        "\n",
        "# Clean text column (remove 'bullet points' and extra whitespace)\n",
        "test_df['text'] = test_df['text'].str.replace('bullet points', '', regex=False)\n",
        "test_df['text'] = test_df['text'].str.replace(r'[\\n\\r\\t]+', ' ', regex=True).str.strip()\n",
        "\n",
        "# Select features for prediction\n",
        "X_test_final = test_df[['text', 'quantity_converted', 'unit_base_encoded']]\n",
        "\n",
        "# Make predictions using the trained model (which includes preprocessing)\n",
        "# Predictions will be on the log_price scale because the model was trained on log_price\n",
        "test_preds_log = model.predict(X_test_final)\n",
        "\n",
        "# Inverse transform the predictions back to the original price scale\n",
        "test_preds = np.expm1(test_preds_log)\n",
        "\n",
        "# Add predictions to the test_df\n",
        "test_df['predicted_price'] = test_preds\n",
        "\n",
        "# display(test_df.head())"
      ],
      "metadata": {
        "id": "3hozUL0YPo0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingdf = test_df.copy()\n",
        "column = [ 'catalog_content', 'image_link', 'value', 'unit', 'text',\n",
        "       'unit_normalized', 'quantity_converted', 'unit_base',\n",
        "       'unit_base_encoded']\n",
        "testingdf = testingdf.drop(columns = column) # Removed index = False"
      ],
      "metadata": {
        "id": "9dPuJg5iPo2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingdf = testingdf.rename(columns={'predicted_price': 'price'})\n",
        "display(testingdf.head())"
      ],
      "metadata": {
        "id": "YDY3SxQDU4D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-mKxqC7U4Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ox-Vz_bJU4Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdHOx2efPo4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHOBcS90Po7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFp2LzFJPo-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eAHJvgQWh-1R"
      },
      "outputs": [],
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# import numpy as np\n",
        "\n",
        "# # Select features\n",
        "# X_text = Df['text'].tolist()\n",
        "# X_numerical = Df[['quantity_converted', 'unit_base_encoded']]\n",
        "# y = Df['price']\n",
        "\n",
        "# # Split data\n",
        "# X_text_train, X_text_test, X_numerical_train, X_numerical_test, y_train, y_test = train_test_split(\n",
        "#     X_text, X_numerical, y, test_size=0.35, random_state=42\n",
        "# )\n",
        "\n",
        "# # Embed text data\n",
        "# model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# X_text_train_embed = model_embed.encode(X_text_train, show_progress_bar=True)\n",
        "# X_text_test_embed = model_embed.encode(X_text_test, show_progress_bar=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb0N96QUiqoq"
      },
      "outputs": [],
      "source": [
        "# # Scale numerical data\n",
        "# scaler = StandardScaler()\n",
        "# X_numerical_train_scaled = scaler.fit_transform(X_numerical_train)\n",
        "# X_numerical_test_scaled = scaler.transform(X_numerical_test)\n",
        "\n",
        "\n",
        "# # Concatenate text embeddings and numerical features\n",
        "# X_train_combined = np.concatenate((X_text_train_embed, X_numerical_train_scaled), axis=1)\n",
        "# X_test_combined = np.concatenate((X_text_test_embed, X_numerical_test_scaled), axis=1)\n",
        "\n",
        "# # Train the Ridge model\n",
        "# reg = Ridge().fit(X_train_combined, y_train)\n",
        "\n",
        "# # Make predictions\n",
        "# preds = reg.predict(X_test_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzI7gMqQqsPJ"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "# from datasets import Dataset\n",
        "# import pandas as pd # Ensure pandas is imported if not already\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# # Include numerical features in the dataset\n",
        "# dataset = Dataset.from_pandas(Df[['text', 'price', 'quantity_converted', 'unit_base_encoded']])\n",
        "\n",
        "# dataset = dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length', max_length=256), batched=True)\n",
        "\n",
        "# dataset = dataset.rename_column(\"price\", \"labels\")\n",
        "# # Ensure numerical columns are kept in the format\n",
        "# dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'quantity_converted', 'unit_base_encoded'])\n",
        "\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)\n",
        "\n",
        "# # Note: To effectively use 'quantity_converted' and 'unit_base_encoded' with this model\n",
        "# # and Trainer, you would typically need to modify the model's forward pass\n",
        "# # to accept and process these numerical features alongside text embeddings.\n",
        "# # This would involve custom model architecture changes beyond adding columns to the dataset.\n",
        "\n",
        "# args = TrainingArguments(output_dir='./results', num_train_epochs=3, per_device_train_batch_size=8)\n",
        "# trainer = Trainer(model=model, args=args, train_dataset=dataset)\n",
        "\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXY59-1ZCeU4"
      },
      "outputs": [],
      "source": [
        "# y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnvyqLaAIt5v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}